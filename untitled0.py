# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NKsJYI-58XJFvVXEvQ4B6YjVQqG8cKQi
"""

# retrain_and_eval.py
import ast
import json
import os
import torch
import numpy as np
import pandas as pd
from torch.utils.data import DataLoader, random_split
from torchvision import datasets, transforms
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns   # optional, used only for nicer cm plot

from model import SimpleCNN
from trainer import train_epoch, validate
from utils import count_params, now

device = "cuda" if torch.cuda.is_available() else "cpu"

# ---------------------------
# Helper: load best config
# ---------------------------
def load_best_config(path="results/search_results.csv"):
    if not os.path.exists(path):
        raise FileNotFoundError(f"No results file at {path}. Run search first.")
    df = pd.read_csv(path)
    # column 'val_acc' expected
    if "val_acc" not in df.columns:
        # maybe JSON format; try to read json
        df = pd.read_json(path.replace(".csv", ".json"))
    # ensure config column parsed
    df_sorted = df.sort_values("val_acc", ascending=False).reset_index(drop=True)
    best_row = df_sorted.loc[0]
    config = best_row["config"]
    # config may be string -> parse
    if isinstance(config, str):
        try:
            config = ast.literal_eval(config)
        except Exception:
            # try json loads
            config = json.loads(config)
    return config, float(best_row["val_acc"]), int(best_row.get("params", 0))


# ---------------------------
# Dataset loaders
# ---------------------------
def get_dataloaders(batch_size=128, val_split=5000):
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))
    ])
    trainset = datasets.CIFAR10(root="./data", train=True, download=True, transform=transform)
    testset = datasets.CIFAR10(root="./data", train=False, download=True, transform=transform)

    train_subset, val_subset = random_split(trainset, [len(trainset)-val_split, val_split], generator=torch.Generator().manual_seed(42))
    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_subset, batch_size=batch_size, num_workers=2)
    test_loader = DataLoader(testset, batch_size=batch_size, num_workers=2)
    return train_loader, val_loader, test_loader


# ---------------------------
# Optimizer builder
# ---------------------------
def get_optimizer(model, config):
    opt = config.get("optimizer", "adam")
    lr = config.get("lr", 0.001)
    if opt == "sgd":
        return torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=config.get("weight_decay", 0.0))
    else:
        return torch.optim.Adam(model.parameters(), lr=lr, weight_decay=config.get("weight_decay", 0.0))


# ---------------------------
# Retrain + Test
# ---------------------------
def retrain_and_evaluate(save_dir="results/final", epochs=50, batch_size=128):
    os.makedirs(save_dir, exist_ok=True)

    config, val_acc, params = load_best_config()
    print("Loaded best config (from search):")
    print(config)
    print(f"previous val_acc: {val_acc:.4f}  params: {params}")

    train_loader, val_loader, test_loader = get_dataloaders(batch_size=batch_size)

    # Build model
    model = SimpleCNN(config["conv_layers"], dropout=config.get("dropout", 0.5)).to(device)
    optimizer = get_optimizer(model, config)

    history = {"epoch":[], "train_loss":[], "train_acc":[], "val_loss":[], "val_acc":[]}

    best_val_acc = 0.0
    best_epoch = 0
    for e in range(1, epochs+1):
        tr_loss, tr_acc = train_epoch(model, train_loader, optimizer, device)
        val_loss, val_accuracy = validate(model, val_loader, device)

        print(f"[{e}/{epochs}] Train Acc: {tr_acc:.4f}  Val Acc: {val_accuracy:.4f}")

        history["epoch"].append(e)
        history["train_loss"].append(tr_loss)
        history["train_acc"].append(tr_acc)
        history["val_loss"].append(val_loss)
        history["val_acc"].append(val_accuracy)

        # save best
        if val_accuracy > best_val_acc:
            best_val_acc = val_accuracy
            best_epoch = e
            torch.save(model.state_dict(), os.path.join(save_dir, "best_model.pth"))
            print(f"  -> New best model saved at epoch {e} with val_acc {best_val_acc:.4f}")

    # Save history
    hist_path = os.path.join(save_dir, "history.json")
    with open(hist_path, "w") as f:
        json.dump(history, f, indent=4)

    # Load best model for test
    model.load_state_dict(torch.load(os.path.join(save_dir, "best_model.pth"), map_location=device))
    model.to(device)
    model.eval()

    # Evaluate on test set and compute confusion matrix
    all_preds = []
    all_labels = []
    with torch.no_grad():
        for x,y in test_loader:
            x = x.to(device)
            out = model(x)
            preds = out.argmax(1).cpu().numpy()
            all_preds.append(preds)
            all_labels.append(y.numpy())

    y_pred = np.concatenate(all_preds)
    y_true = np.concatenate(all_labels)

    test_acc = (y_pred == y_true).mean()
    print(f"\nFinal Test Accuracy: {test_acc:.4f}")

    # Confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    # Save cm as numpy
    np.save(os.path.join(save_dir, "confusion_matrix.npy"), cm)

    # Save classification report
    report = classification_report(y_true, y_pred, output_dict=True)
    with open(os.path.join(save_dir, "classification_report.json"), "w") as f:
        json.dump(report, f, indent=4)

    # Optionally plot confusion matrix
    try:
        plt.figure(figsize=(9,7))
        sns.heatmap(cm, annot=True, fmt="d")
        plt.title(f"Confusion Matrix (test) - Acc {test_acc:.4f}")
        plt.xlabel("Predicted")
        plt.ylabel("True")
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, "confusion_matrix.png"))
        plt.close()
    except Exception as ex:
        print("Could not plot confusion matrix (matplotlib/seaborn issue):", ex)

    # Save metadata
    meta = {
        "best_val_acc": best_val_acc,
        "best_epoch": best_epoch,
        "final_test_acc": float(test_acc),
        "params": count_params(model),
        "timestamp": now(),
        "config": config
    }
    with open(os.path.join(save_dir, "meta.json"), "w") as f:
        json.dump(meta, f, indent=4)

    print("Retrain + evaluation complete. Artifacts saved in:", save_dir)
    return meta


if _name_ == "_main_":
    # Change epochs if you have GPU; if CPU, keep small (e.g., 20)
    retrain_and_evaluate(save_dir="results/final", epochs=40,Â batch_size=128)